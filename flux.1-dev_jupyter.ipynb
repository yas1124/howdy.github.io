{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yas1124/howdy.github.io/blob/main/flux.1-dev_jupyter.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Google Driveをマウント\n",
        "from google.colab import drive\n",
        "import os\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "save_dir = \"/content/drive/MyDrive/FLUX_outputs\"\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "# 2. 本体（TotoroUI）のセットアップ\n",
        "%cd /content\n",
        "!rm -rf /content/TotoroUI\n",
        "!git clone -b totoro3 https://github.com/camenduru/ComfyUI /content/TotoroUI\n",
        "%cd /content/TotoroUI\n",
        "\n",
        "# 3. 依存ライブラリのクリーンインストール\n",
        "# 一旦問題を起こしている torchvision を消して、適切なバージョンを入れ直します\n",
        "!pip uninstall -y torchvision torchaudio\n",
        "!pip install -q torchsde einops diffusers accelerate xformers==0.0.28.post2\n",
        "!pip install -q torchvision==0.20.0 --no-deps\n",
        "!apt -y install -qq aria2\n",
        "\n",
        "# 4. モデルのダウンロード\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/camenduru/FLUX.1-dev/resolve/main/flux1-dev-fp8.safetensors -d /content/TotoroUI/models/unet -o flux1-dev-fp8.safetensors\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/camenduru/FLUX.1-dev/resolve/main/ae.sft -d /content/TotoroUI/models/vae -o ae.sft\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/camenduru/FLUX.1-dev/resolve/main/clip_l.safetensors -d /content/TotoroUI/models/clip -o clip_l.safetensors\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/camenduru/FLUX.1-dev/resolve/main/t5xxl_fp8_e4m3fn.safetensors -d /content/TotoroUI/models/clip -o t5xxl_fp8_e4m3fn.safetensors\n",
        "\n",
        "# 5. インポートと初期化\n",
        "import sys\n",
        "sys.path.append('/content/TotoroUI')\n",
        "import torch\n",
        "import numpy as np\n",
        "import random\n",
        "from PIL import Image\n",
        "from datetime import datetime\n",
        "\n",
        "import nodes\n",
        "from nodes import NODE_CLASS_MAPPINGS\n",
        "from totoro_extras import nodes_custom_sampler\n",
        "from totoro import model_management\n",
        "\n",
        "DualCLIPLoader = NODE_CLASS_MAPPINGS[\"DualCLIPLoader\"]()\n",
        "UNETLoader = NODE_CLASS_MAPPINGS[\"UNETLoader\"]()\n",
        "RandomNoise = nodes_custom_sampler.NODE_CLASS_MAPPINGS[\"RandomNoise\"]()\n",
        "BasicGuider = nodes_custom_sampler.NODE_CLASS_MAPPINGS[\"BasicGuider\"]()\n",
        "KSamplerSelect = nodes_custom_sampler.NODE_CLASS_MAPPINGS[\"KSamplerSelect\"]()\n",
        "BasicScheduler = nodes_custom_sampler.NODE_CLASS_MAPPINGS[\"BasicScheduler\"]()\n",
        "SamplerCustomAdvanced = nodes_custom_sampler.NODE_CLASS_MAPPINGS[\"SamplerCustomAdvanced\"]()\n",
        "VAELoader = NODE_CLASS_MAPPINGS[\"VAELoader\"]()\n",
        "VAEDecode = NODE_CLASS_MAPPINGS[\"VAEDecode\"]()\n",
        "EmptyLatentImage = NODE_CLASS_MAPPINGS[\"EmptyLatentImage\"]()\n",
        "\n",
        "def closestNumber(n, m):\n",
        "    q = int(n / m)\n",
        "    n1 = m * q\n",
        "    if (n * m) > 0:\n",
        "        n2 = m * (q + 1)\n",
        "    else:\n",
        "        n2 = m * (q - 1)\n",
        "    if abs(n - n1) < abs(n - n2):\n",
        "        return n1\n",
        "    return n2\n",
        "\n",
        "with torch.inference_mode():\n",
        "    clip = DualCLIPLoader.load_clip(\"t5xxl_fp8_e4m3fn.safetensors\", \"clip_l.safetensors\", \"flux\")[0]\n",
        "    unet = UNETLoader.load_unet(\"flux1-dev-fp8.safetensors\", \"fp8_e4m3fn\")[0]\n",
        "    vae = VAELoader.load_vae(\"ae.sft\")[0]\n",
        "\n",
        "print(\"--- 準備完了！ステップ2を実行してください ---\")"
      ],
      "metadata": {
        "id": "z-UIl3d56J-m",
        "outputId": "66934271-46f8-468b-a1d2-2334fd698efc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "/content\n",
            "Cloning into '/content/TotoroUI'...\n",
            "remote: Enumerating objects: 20484, done.\u001b[K\n",
            "remote: Total 20484 (delta 0), reused 0 (delta 0), pack-reused 20484 (from 1)\u001b[K\n",
            "Receiving objects: 100% (20484/20484), 65.43 MiB | 26.60 MiB/s, done.\n",
            "Resolving deltas: 100% (13661/13661), done.\n",
            "/content/TotoroUI\n",
            "Found existing installation: torchvision 0.24.0+cu126\n",
            "Uninstalling torchvision-0.24.0+cu126:\n",
            "  Successfully uninstalled torchvision-0.24.0+cu126\n",
            "Found existing installation: torchaudio 2.9.0+cu126\n",
            "Uninstalling torchaudio-2.9.0+cu126:\n",
            "  Successfully uninstalled torchaudio-2.9.0+cu126\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.7/16.7 MB\u001b[0m \u001b[31m117.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m906.4/906.4 MB\u001b[0m \u001b[31m194.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.inference_mode():\n",
        "    # --- 設定項目 ---\n",
        "    # 添付画像を元にしたプロンプト\n",
        "    positive_prompt = \"\"\"\n",
        "    Cinematic, hyper-realistic photo of a beautiful young Japanese woman standing waist-deep in a crystal-clear tropical lagoon,\n",
        "    reaching out one hand toward the viewer in a warm, welcoming gesture. She is wearing an ethereal fantasy dress made of\n",
        "    flowing light pink fabric, intricately adorned with iridescent seashells, glowing pearls, and delicate coral textures.\n",
        "    Beneath the shimmering water surface, vibrant coral reefs and colorful tropical fish are visible.\n",
        "    The background features a serene white-sand beach with swaying palm trees under a bright, golden afternoon sun.\n",
        "    High detail, 8k, emotional and hopeful atmosphere, soft natural bokeh, masterpiece.\n",
        "    \"\"\"\n",
        "\n",
        "    width = 1024\n",
        "    height = 1024\n",
        "    seed = 0         # 0でランダム生成\n",
        "    steps = 30       # ご指定のステップ数\n",
        "    guidance = 3.5   # ご指定のCFGスケール (Flux Guidance)\n",
        "\n",
        "    sampler_name = \"euler\"\n",
        "    scheduler = \"simple\"\n",
        "    # ---------------\n",
        "\n",
        "    if seed == 0:\n",
        "        seed = random.randint(0, 18446744073709551615)\n",
        "    print(f\"生成開始 (Steps: {steps}, Guidance: {guidance}, Seed: {seed})\")\n",
        "\n",
        "    # 1. プロンプトのエンコード\n",
        "    cond, pooled = clip.encode_from_tokens(clip.tokenize(positive_prompt), return_pooled=True)\n",
        "\n",
        "    # Guidance（CFGスケール）の設定を条件データに追加\n",
        "    cond = [[cond, {\"pooled_output\": pooled, \"guidance\": guidance}]]\n",
        "\n",
        "    # 2. 生成プロセスの実行\n",
        "    noise = RandomNoise.get_noise(seed)[0]\n",
        "    guider = BasicGuider.get_guider(unet, cond)[0]\n",
        "    sampler = KSamplerSelect.get_sampler(sampler_name)[0]\n",
        "    sigmas = BasicScheduler.get_sigmas(unet, scheduler, steps, 1.0)[0]\n",
        "\n",
        "    latent_image = EmptyLatentImage.generate(closestNumber(width, 16), closestNumber(height, 16))[0]\n",
        "    sample, sample_denoised = SamplerCustomAdvanced.sample(noise, guider, sampler, sigmas, latent_image)\n",
        "\n",
        "    model_management.soft_empty_cache()\n",
        "\n",
        "    # 3. 画像の復元と保存\n",
        "    decoded = VAEDecode.decode(vae, sample)[0].detach()\n",
        "    final_image = Image.fromarray(np.array(decoded*255, dtype=np.uint8)[0])\n",
        "\n",
        "    # ファイル名作成（日時+シード）\n",
        "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    filename = f\"flux_fantasy_{timestamp}_{seed}.png\"\n",
        "    save_path = os.path.join(save_dir, filename)\n",
        "\n",
        "    # Google Driveに保存\n",
        "    final_image.save(save_path)\n",
        "    print(f\"完了！Google Driveに保存されました: {save_path}\")\n",
        "\n",
        "# 表示\n",
        "final_image"
      ],
      "metadata": {
        "id": "CUp7rg7SA8u4",
        "outputId": "da50849c-83cf-4e4c-faca-ac46a79a8aa8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'torch' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2849805845.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minference_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;31m# --- 設定項目 ---\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;31m# 添付画像を元にしたプロンプト\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     positive_prompt = \"\"\"\n\u001b[1;32m      5\u001b[0m     \u001b[0mCinematic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhyper\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mrealistic\u001b[0m \u001b[0mphoto\u001b[0m \u001b[0mof\u001b[0m \u001b[0ma\u001b[0m \u001b[0mbeautiful\u001b[0m \u001b[0myoung\u001b[0m \u001b[0mJapanese\u001b[0m \u001b[0mwoman\u001b[0m \u001b[0mstanding\u001b[0m \u001b[0mwaist\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mdeep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0ma\u001b[0m \u001b[0mcrystal\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mclear\u001b[0m \u001b[0mtropical\u001b[0m \u001b[0mlagoon\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}